{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7339cd41",
   "metadata": {},
   "source": [
    "# RAG End to End Pipeline with ElasticSearch and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62324a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:53:01.201243Z",
     "start_time": "2024-06-03T17:52:52.825297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: elasticsearch in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (8.13.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from elasticsearch) (8.13.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.2.2)\n",
      "Requirement already satisfied: openai in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (1.30.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (2.7.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\padma\\anaconda3\\envs\\rag\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install elasticsearch\n",
    "!pip install openai\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca5fca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:56:29.766060Z",
     "start_time": "2024-06-14T10:56:29.162762Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ELAST_SEARCH_PWD = os.getenv('ELASTIC_SEARCH_PWD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c62533",
   "metadata": {},
   "source": [
    "## Data fetching and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0461a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:56:42.325090Z",
     "start_time": "2024-06-14T10:56:30.923470Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "def prepare_data(name):\n",
    "    dataset = load_dataset(name)\n",
    "\n",
    "    # Convert the dataset to a pandas DataFrame\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    \n",
    "    # Extract each row as a dictionary and compile into a list of document\n",
    "    doc_list = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        doc_dict = row.to_dict()\n",
    "        doc_list.append(doc_dict)\n",
    "    return doc_list\n",
    "    \n",
    "    \n",
    "doc_list = prepare_data(\"microsoft/wiki_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be3da0",
   "metadata": {},
   "source": [
    "## Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c30a31",
   "metadata": {},
   "source": [
    "Start ElasticSearch server before this step via the docker compose file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a8c7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:56:56.752150Z",
     "start_time": "2024-06-14T10:56:56.553607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '0eb7766eedf4', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'o24P37j-SCy8ibGvjUXlAQ', 'version': {'number': '8.13.4', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'da95df118650b55a500dcc181889ac35c6d8da7c', 'build_date': '2024-05-06T22:04:45.107454559Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", basic_auth=('elastic', ELAST_SEARCH_PWD))\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b8ab4",
   "metadata": {},
   "source": [
    "### Store Data in ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94f4c52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:01:53.074942Z",
     "start_time": "2024-06-14T10:56:57.813579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c700a7a27f734e0fa95409f4d8345239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def index_document(index_name, documents):\n",
    "    \"\"\"\n",
    "    Index documents into Elasticsearch. If the index doesn't exist, it creates one.\n",
    "    Loop through each document and index it into Elasticsearch using the question_id as a unique identifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the settings and mappings for the Elasticsearch index\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"answer\": {\"type\": \"text\"},\n",
    "                \"document_title\": {\"type\": \"text\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"question_id\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if the index exists and create it if it does not\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name, body=index_settings)\n",
    "        print(\"Index created\")\n",
    "    else:\n",
    "        print(\"Index already exists\")\n",
    "    \n",
    "    # Index each document\n",
    "    for doc in tqdm(documents):\n",
    "        doc_id = doc['question_id']  # Using 'question_id' as the unique identifier\n",
    "        es.index(index=index_name, id=doc_id, document=doc)\n",
    "        \n",
    "        \n",
    "index_name = \"wiki_qa_questions\"\n",
    "index_document(index_name,doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b997d1",
   "metadata": {},
   "source": [
    "### Search the stored data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89bdf81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:09:13.649501Z",
     "start_time": "2024-06-14T11:09:13.638620Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, index_name, max_results=5):\n",
    "    \"\"\"\n",
    "    Retrieve documents from an Elasticsearch index based on a search query.\n",
    "    Returns the top results up to a specified maximum number.\n",
    "    \"\"\"\n",
    "    \n",
    "    es = Elasticsearch(\"http://localhost:9200\", basic_auth=('elastic', ELAST_SEARCH_PWD))\n",
    "    \n",
    "    search_query = {\n",
    "        \"size\": max_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,    # The actual search query text\n",
    "                        \"fields\": [\"question^3\", \"answer\", \"document_title\"],  # Fields to search in with boosting 'question' field\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Perform the search with the defined query\n",
    "    response = es.search(index=index_name, body=search_query)\n",
    "    \n",
    "    # Extract and return the document sources from the search hits\n",
    "    documents = [hit['_source'] for hit in response['hits']['hits']]\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6630f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:09:15.336636Z",
     "start_time": "2024-06-14T11:09:15.066599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question_id': 'Q8', 'question': 'How are epithelial tissues joined together?', 'document_title': 'Tissue (biology)', 'answer': 'With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of clinical diagnosis and prognosis .', 'label': 0}, {'question_id': 'Q277', 'question': 'what films has hugh grant and richard curtis done together', 'document_title': 'Hugh Grant', 'answer': 'In a career spanning 30 years, Grant has repeatedly claimed that acting is not a true calling but just a job he fell into.', 'label': 0}, {'question_id': 'Q2508', 'question': 'What is the function of an epithelial free surface that is smooth?', 'document_title': 'Epithelium', 'answer': 'Epithelia can also be organized into clusters of cells that function as exocrine and endocrine glands.', 'label': 0}, {'question_id': 'Q41', 'question': 'how cds are read', 'document_title': 'CD-ROM', 'answer': 'It adapted the format to hold any form of data.', 'label': 0}, {'question_id': 'Q87', 'question': 'how are sheep slaughtered', 'document_title': 'Animal slaughter', 'answer': 'The animals most commonly slaughtered for food are cattle , water buffalo (for beef and veal ), sheep , goats (for lamb and mutton ), pigs (for pork ), horses (for horse meat ), and fowl , largely chickens , turkeys , and ducks and increasingly fish from the aquaculture industry ( fish farming ).', 'label': 0}]\n",
      "Title: Tissue (biology)\n",
      "Question: How are epithelial tissues joined together?\n",
      "Answer: With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of clinical diagnosis and prognosis .\n",
      "\n",
      "\n",
      "Title: Hugh Grant\n",
      "Question: what films has hugh grant and richard curtis done together\n",
      "Answer: In a career spanning 30 years, Grant has repeatedly claimed that acting is not a true calling but just a job he fell into.\n",
      "\n",
      "\n",
      "Title: Epithelium\n",
      "Question: What is the function of an epithelial free surface that is smooth?\n",
      "Answer: Epithelia can also be organized into clusters of cells that function as exocrine and endocrine glands.\n",
      "\n",
      "\n",
      "Title: CD-ROM\n",
      "Question: how cds are read\n",
      "Answer: It adapted the format to hold any form of data.\n",
      "\n",
      "\n",
      "Title: Animal slaughter\n",
      "Question: how are sheep slaughtered\n",
      "Answer: The animals most commonly slaughtered for food are cattle , water buffalo (for beef and veal ), sheep , goats (for lamb and mutton ), pigs (for pork ), horses (for horse meat ), and fowl , largely chickens , turkeys , and ducks and increasingly fish from the aquaculture industry ( fish farming ).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How are epithelial tissues joined together?\"\n",
    "\n",
    "response = retrieve_documents(query=user_question,index_name=\"wiki_qa_questions\")\n",
    "print(response)\n",
    "\n",
    "for doc in response:\n",
    "    print(f\"Title: {doc['document_title']}\\nQuestion: {doc['question']}\\nAnswer: {doc['answer']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016704d8",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce90d593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:09:21.173078Z",
     "start_time": "2024-06-14T11:09:21.157112Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Set the OpenAI API key for accessing the API\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def build_context(documents):\n",
    "    \"\"\"\n",
    "    Create a string of information from documents to help the AI understand the question.\n",
    "    This string includes document titles, questions, and answers.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for doc in documents:\n",
    "        doc_str = f\"Title: {doc['document_title']}\\nQuestion: {doc['question']}\\nAnswer: {doc['answer']}\\n\\n\"\n",
    "        context += doc_str\n",
    "    return context.strip()\n",
    "\n",
    "def augment_prompt(user_question, documents):\n",
    "    \"\"\"\n",
    "    Create a message for the AI model that includes the user's question and detailed context\n",
    "    from documents. This helps the AI provide a more accurate answer.\n",
    "    \"\"\"\n",
    "    context = build_context(documents)\n",
    "    return f\"\"\"\n",
    "    You're an AI assistant.\n",
    "    Answer the user QUESTION based on CONTEXT - the documents retrieved from our FAQ database.\n",
    "    Don't use other information outside of the provided CONTEXT.\n",
    "\n",
    "    QUESTION: {user_question}\n",
    "\n",
    "    CONTEXT:\n",
    "\n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def generate_response(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Send the prompt to OpenAI and get the model's response. This uses detailed context\n",
    "    to improve the quality of the AI's answer.\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a4ec0",
   "metadata": {},
   "source": [
    "### End-to-End RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5923618b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:09:23.961003Z",
     "start_time": "2024-06-14T11:09:22.409344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epithelial tissues are joined together in clusters of cells that can function as exocrine and endocrine glands.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qa_bot(user_question, index_name):\n",
    "    \"\"\"\n",
    "    Orchestrate the full RAG flow: retrieve relevant documents, construct a prompt based on these\n",
    "    documents, and query the AI model to generate a response. This function encapsulates the end-to-end\n",
    "    process of a retrieval-augmented generation system.\n",
    "    \"\"\"\n",
    "    context_docs = retrieve_documents(user_question, index_name)\n",
    "    prompt = augment_prompt(user_question, context_docs)\n",
    "    return generate_response(prompt)\n",
    "\n",
    "\n",
    "\n",
    "# Define the index name and user question\n",
    "index_name = \"wiki_qa_questions\"\n",
    "user_question = \"How are epithelial tissues joined together?\"\n",
    "\n",
    "# Call the QA bot to get the answer\n",
    "qa_bot(user_question, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e4965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
